{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Engineering Capstone Project\n",
    "### Process immegration data and co-relate with temperature data using Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_im = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = df_im\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "def get_temp_data(path=\"../../data2/GlobalLandTemperaturesByCity.csv\"):\n",
    "    return spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "\n",
    "def get_imm_data(path='../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'):\n",
    "    return spark.read.format('com.github.saurfang.sas.spark').load(path)\n",
    "\n",
    "# Test\n",
    "# get_temp_data().show()\n",
    "# get_imm_data().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "1. By checking the data using pandas, it was observed that AverageTemperature data might contains NAN or \"\"\n",
    "2. The City in the temperature data and City in the I94_SAS_Labels_Description SAS file doesn't always match \n",
    "2. The i94port in immigration data contains XXX \n",
    "\n",
    "#### Cleaning Steps\n",
    "Clean temperature data with AverageTemperature with NAN and \"\"\n",
    "Clean immigration data with i94port null and XXX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_temp_data(df_temp_data):\n",
    "    df_temp_data=df_temp_data.filter(df_temp_data.AverageTemperature != 'NaN')\n",
    "    df_temp_data=df_temp_data.filter(df_temp_data.AverageTemperature != \"\")\n",
    "    df_temp_data=df_temp_data.withColumn(\"i94_port\", map_i94port(df_temp_data.City))\n",
    "    df_temp_data=df_temp_data.filter(df_temp_data.i94_port != 'null')\n",
    "    return df_temp_data\n",
    "\n",
    "def clean_imm_data(imm_data):\n",
    "    imm_data=imm_data.filter((imm_data.i94port != 'null') & (imm_data.i94port != 'xxx'))\n",
    "    return imm_data\n",
    "\n",
    "# Test\n",
    "# clean_temp_data(get_temp_data())\n",
    "# clean_imm_data(get_imm_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Pre pipeline Mapping and definition\n",
    "1. Create the City to port mapping data using the data fiven in I94_SAS_Labels_Descriptions.SAS file\n",
    "   Upon manual inspection it is found that the mapping data recides from line 303:893\n",
    "   output will be a dictionary with {City: I94_port} schema\n",
    "2. Create a UDF to map the city to i94 port using the mapping data created using the above step\n",
    "   Upon manaul inspection, its found that the city in temperature data is not accurately mathcing with the \n",
    "   city in immigration data, hence the mapping is done using simple substring search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_city_map = {}\n",
    "with open(\"I94_SAS_Labels_Descriptions.SAS\") as fd:\n",
    "    lines = fd.readlines()\n",
    "    port_city_map = {x.split(\"'\")[3].strip().lower(): x.split(\"'\")[1] for x in lines[303:893]}\n",
    "\n",
    "@udf()\n",
    "def map_i94port(col):\n",
    "      for key, value in port_city_map.items():\n",
    "        if col.lower() in key:\n",
    "            return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_imm_temp_fact_df(df_imm, df_temp_data):\n",
    "    df_imm.createOrReplaceTempView(\"imm_data_table\")\n",
    "    df_temp_data.createOrReplaceTempView(\"temp_data_table\")\n",
    "\n",
    "    fact_table = spark.sql('''\n",
    "    SELECT\n",
    "           i94port,\n",
    "           arrdate,\n",
    "           depdate,\n",
    "           i94visa as visa_type,\n",
    "           AverageTemperature as temp,\n",
    "           i94yr as year,\n",
    "           i94mon as month,\n",
    "           i94cit as city\n",
    "    FROM imm_data_table JOIN temp_data_table ON imm_data_table.i94port = temp_data_table.i94_port\n",
    "    ''')\n",
    "    fact_table.createOrReplaceTempView(\"temp_imm_data_table\")\n",
    "    return fact_table\n",
    "\n",
    "def save_df_parquet(df_imm, df_temp_data, fact_table):\n",
    "    # Save all data as parquet files\n",
    "    fact_table.write.mode('overwrite').partitionBy(\"i94port\", \"visa_type\").parquet( \"i94_temp_fact.parquet\")\n",
    "    df_imm.write.mode('overwrite').partitionBy(\"i94port\", \"i94visa\").parquet( \"i94_imm_dim.parquet\")\n",
    "    df_temp_data.write.mode('overwrite').partitionBy(\"i94_port\", \"City\").parquet( \"port_temp_dim.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "temp_data = get_temp_data()\n",
    "imm_data = get_imm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean data\n",
    "temp_data = clean_temp_data(temp_data)\n",
    "imm_data = clean_imm_data(imm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load fact data\n",
    "fact_data = get_imm_temp_fact_df(imm_data, temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10644246266"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save all data\n",
    "save_df_parquet(imm_data, temp_data, fact_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_quality_check(test):\n",
    "    count = test['df'].count()\n",
    "    if count != test['expected']:\n",
    "        print(f\"[FAIL]: Data quality check failed for {test['name']} with {count} records, expected {test['expected']}\")\n",
    "        return\n",
    "    print(f\"[PASS]: Data quality check failed for {test['name']} with {count} records\")\n",
    "\n",
    "test_cases = [\n",
    "    {\"name\": \"imm_data\", \"df\":imm_data, \"expected\": 3096313},\n",
    "    {\"name\": \"temp_data\", \"df\":temp_data, \"expected\": 540912},\n",
    "    {\"name\": \"fact_data\", \"df\":fact_data, \"expected\": 10644246266}\n",
    "]\n",
    "for test in test_cases:\n",
    "    data_quality_check(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Below mentioned dictinoary provides the idea of data with description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Immigration_temperature_fact_schema = {\n",
    "    \"year\" : \"year of temperature recorded\",\n",
    "    \"month\": \"month of temperature recorded\",\n",
    "    \"City\": \"city code\",\n",
    "    \"i94port\": \"destination us city i94 port code\",\n",
    "    \"arrdate\": \"arrival date in the USA\",\n",
    "    \"depdate\": \"dearture date in the USA\",\n",
    "    \"visa_type\": \"Type of visa\",\n",
    "}\n",
    "\n",
    "temperature_data_schema = {\n",
    "    \"AverageTemperature\": \"average temperature\",\n",
    "    \"City\": \"city name\",\n",
    "    \"Country\": \"country name\",\n",
    "    \"Latitude\": \"latitude\",\n",
    "    \"Longitude\": \"longitude\"\n",
    "}\n",
    "\n",
    "Immegration_data_schema = {\n",
    "    \"i94yr\" = \"year\",\n",
    "    \"i94mon\" =  \"month\",\n",
    "    \"i94cit\" = \"city\",\n",
    "    \"i94port\" = \"I94 port code\",\n",
    "    \"arrdate\" = \"arrival date\",\n",
    "    \"i94mode\" = \"travel mode\",\n",
    "    \"depdate\" = \"departure date\",\n",
    "    \"i94visa\" = \"Type of visa\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "1. Pandas is used to play around the data as it gives developers the power to quickly read the files and play around the data before even implementing it using the Spark on large datasets\n",
    "2. Spark is used as it can compute complex and resource consuming computations in-memory and distributed with reliablity, also it can handle all variety of data. Also Spark SQL helps to process the large input files into dataframes and manipulated via standard SQL join operations to form additional tables.\n",
    "\n",
    "### Propose how often the data should be updated and why.\n",
    "\n",
    "The data should be updated weekly as temperature changes and people will be travelling the country every day.\n",
    "\n",
    "### Write a description of how you would approach the problem differently under the following scenarios:\n",
    "     1. The data was increased by 100x.\n",
    "\n",
    "    Spark should be running in cluster mode using a cluster manager like Mesos or yarm.\n",
    "     Monitoring will be necessary to keep track of the data being processed\n",
    "    Spark jobs can be schedules using Apache Airflow to run the spark processing every night or couple of hours before 7AM.\n",
    "    All the parquet files cam be made available in query tools like, PIG/Hive or Impala like tools to access the data by 100+ people\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
